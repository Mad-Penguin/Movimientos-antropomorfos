<!DOCTYPE html><html lang="es-ES" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Mocap" /><meta property="og:locale" content="es_ES" /><meta name="description" content="La captura de movimiento (mocap abreviado del inglés motion capture) es el proceso de grabar el movimiento de un objeto o personas para generar un modelo 3D que pueda ser ejecutado por la computadora, estos son clasificados en outside-in, inside-out e inside-in dependiendo de las fuentes de captura y de donde son puestos los sensores o marcadores." /><meta property="og:description" content="La captura de movimiento (mocap abreviado del inglés motion capture) es el proceso de grabar el movimiento de un objeto o personas para generar un modelo 3D que pueda ser ejecutado por la computadora, estos son clasificados en outside-in, inside-out e inside-in dependiendo de las fuentes de captura y de donde son puestos los sensores o marcadores." /><link rel="canonical" href="https://mad-penguin.github.io/Movimientos-antropomorfos/mocap/" /><meta property="og:url" content="https://mad-penguin.github.io/Movimientos-antropomorfos/mocap/" /><meta property="og:site_name" content="Movimientos antropomorfos" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-07-29T13:50:59+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Mocap" /><meta name="twitter:site" content="@twitter_username" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-07-29T13:50:59+00:00","datePublished":"2022-07-29T13:50:59+00:00","description":"La captura de movimiento (mocap abreviado del inglés motion capture) es el proceso de grabar el movimiento de un objeto o personas para generar un modelo 3D que pueda ser ejecutado por la computadora, estos son clasificados en outside-in, inside-out e inside-in dependiendo de las fuentes de captura y de donde son puestos los sensores o marcadores.","headline":"Mocap","mainEntityOfPage":{"@type":"WebPage","@id":"https://mad-penguin.github.io/Movimientos-antropomorfos/mocap/"},"url":"https://mad-penguin.github.io/Movimientos-antropomorfos/mocap/"}</script><title>Mocap | Movimientos antropomorfos</title><link rel="apple-touch-icon" sizes="180x180" href="/Movimientos-antropomorfos/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/Movimientos-antropomorfos/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/Movimientos-antropomorfos/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/Movimientos-antropomorfos/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/Movimientos-antropomorfos/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Movimientos antropomorfos"><meta name="application-name" content="Movimientos antropomorfos"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/Movimientos-antropomorfos/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"><link rel="stylesheet" href="/Movimientos-antropomorfos/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/Movimientos-antropomorfos/" class="mx-auto"> <img src="/Movimientos-antropomorfos/assets/images/UG.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/Movimientos-antropomorfos/">Movimientos antropomorfos</a></div><div class="site-subtitle font-italic">Verano UG</div></div><ul class="w-100"><li class="nav-item"> <a href="/Movimientos-antropomorfos/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>INICIO</span> </a><li class="nav-item active"> <a href="/Movimientos-antropomorfos/mocap/" class="nav-link"> <i class="fa-fw fa-solid fa-camera ml-xl-3 mr-xl-3 unloaded"></i> <span>MOCAP</span> </a><li class="nav-item"> <a href="/Movimientos-antropomorfos/modelos/" class="nav-link"> <i class="fa-fw fa-solid fa-user-large ml-xl-3 mr-xl-3 unloaded"></i> <span>MODELOS</span> </a><li class="nav-item"> <a href="/Movimientos-antropomorfos/aplicaciones/" class="nav-link"> <i class="fa-fw fa-solid fa-desktop ml-xl-3 mr-xl-3 unloaded"></i> <span>APLICACIONES</span> </a><li class="nav-item"> <a href="/Movimientos-antropomorfos/bibliografia/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>BIBLIOGRAFIA</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/Movimientos-antropomorfos/"> Inicio </a> </span> <span>Mocap</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Mocap</div><i id="search-trigger" class="fas fa-search fa-fw"></i>  <span id="search-cancel" >Cancelar</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 class="dynamic-title"> Mocap</h1><div class="post-content"><p>La captura de movimiento (<strong>mocap</strong> abreviado del inglés motion capture) es el proceso de grabar el movimiento de un objeto o personas para generar un modelo 3D que pueda ser ejecutado por la computadora, estos son clasificados en outside-in, inside-out e inside-in dependiendo de las fuentes de captura y de donde son puestos los sensores o marcadores.</p><ul><li>Los sistemas outside-in usan sensores externos para capturar los datos de fuentes colocadas en partes del cuerpo, un ejemplo de esto son los dispositivos de rastreo basados en cámaras en el que las cámaras son los sensores y los marcadores reflectantes son las fuentes<li>Los sistemas inside-out son sensores que se ponen en el cuerpo que recolectan fuentes externas, un ejemplo de esto son sistemas electromagnéticos cuyos sensores se mueven en un campo electromagnético generado externamente<li>Los sistemas inside-in tienen tanto los sensores como las fuentes en el cuerpo.</ul><p>Los sistemas ópticos de captura de movimiento son un método muy preciso para capturar ciertos movimientos, es un sistema basado en una computadora que controla la entrada de las cámaras, las cuales son sensibles a la luz para crear las representaciones digitales. Este es el sistema con el que cuenta la universidad del cual se hablara más adelante, primero hay que tratar con el problema del posicionamiento de las cámaras.</p><h1 id="posicionamiento-de-cámaras">Posicionamiento de cámaras</h1><p>Un problema que surge comúnmente en la captura de movimiento es el posicionamiento adecuado de las cámaras. Para saber las coordenadas de un punto de interés, es necesario que este sea visto por al menos dos cámaras (o más, dependiendo del sistema). Así, es de interés poder calcular las posiciones que maximicen la visibilidad de puntos.</p><p>Cómo parte del proyecto, consultamos el paper Optimal Camera Placement for Motion Capture Systems. Dicha publicación propone una nueva manera de calcular posiciones óptimas para las cámaras en un entorno para la captura de movimiento.</p><h2 id="métrica-de-error-basada-en-oclusión"><span class="mr-2">Métrica de error basada en oclusión</span><a href="#métrica-de-error-basada-en-oclusión" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>La <strong>métrica de error basada en oclusión</strong> presenta una forma de medir la visibilidad de un punto objetivo específico por al menos dos cámaras, para un conjunto de cámaras, cuando se presenta oclusión dinámica.</p><p>Para tomar en cuenta la gran mayoría de oclusores posibles, se considera un plano vertical que gira alrededor del punto objetivo. Así, entre todos todos los ángulos entre $0$° y $360$° que puede girar este plano respecto al punto, habrá ciertos intervalos de ángulos en los cuáles el punto no es visible por al menos dos cámaras. La métrica calcula la suma de las longitudes de estos intervalos. Informalmente, esta corresponde a la “suma de ángulos” en los cuáles no es visible el punto objetivo, y nos interesa minimizar esta cantidad.</p><p><img data-src="/Movimientos-antropomorfos/assets/images/oclusion.png" alt="Oclusion" data-proofer-ignore></p><p>En la imagen anterior, observamos que la visibilidad del punto sólo cambia cuando el plano cruza el vector de la vista de una cámara. Notamos también que si existen $n$ cámaras, sus vectores de vista generan $2n$ regiones (es decir, intervalos de ángulos) de interés; por ejemplo, en la imagen, dos cámaras dividen el círculo centrado en el punto de interés en $4$ partes.</p><p>Además de esto, consideramos una <strong>restricción de triangulación</strong> para un par de cámaras: aunque ambas sean capaces de ver el punto de interés; sólo se toman en cuenta si el ángulo entre sus vectores de vista está entre 40° y 140°. Esto es porque si el ángulo es muy grande o muy pequeño, el error numérico al intentar triangular el punto es también muy grande. Asimismo, se descartan las cámaras si el punto está mas allá de el rango efectivo de estas.</p><p>La métrica anterior se puede calcular, en pseudocódigo, con el siguiente algoritmo:</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="¡Copiado!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre><td class="rouge-code"><pre>#n - cantidad de camaras
#C[1..n] - posiciones de las cámaras
#S[1..2n] - las longitudes de las regiones que generan 
los vectores de vista de las camaras
#Cf[] - posiciones de cámaras en C frente al oclusor
#flag - es verdadera si en la región,
el punto es visible por al menos dos cámaras
Q - suma de las regiones en las que no es visible el punto.
por al menos dos cámaras.
-----------------------------------------------------
calcular S 
para i = 1 hasta 2n:
    calcular Cf a partir de C y S[i]
    flag = false
    para j = 1 hasta longitud(Cf):
	    para k = j + 1 to longitud(Cf):
		    si Cf[j] y Cf[k] satisfacen la
		    restricción de triangulación:
			    flag = true
			    break
	si flag = false:
		Q += S[i]
regresa S[i]
</pre></table></code></div></div><h2 id="recocido-simulado"><span class="mr-2">Recocido simulado</span><a href="#recocido-simulado" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Para aplicar la métrica anterior en el cálculo de posiciones óptimas, se utiliza el método de <strong>recocido simulado</strong>. Este es un método muy general que, de manera heurística, busca una aproximación al mínimo o máximo global de una función.</p><p>Este algoritmo considera una <strong>temperatura</strong>. Se inicia en una estado arbitrario; y se consideran varios estados siguientes válidos. Si alguno de estas mejora la función que queremos minimizar (o maximizar), cambiamos a este estado. De otra manera, aunque el siguiente estado empeore la función, con cierta probabilidad que depende de la temperatura, se cambia a este estado.</p><p>La temperatura está definida de tal manera que, entre más alta, es más probable que se tome una nueva posición. Así, se inicia con una temperatura alta, la cual va disminuyendo conforme pasa el tiempo. El objetivo de esta es “escapar” mínimos locales; es decir, estados que no son óptimos, pero que están rodeados de estados peores.</p><p>Volviendo al problema de posicionamiento de cámaras, consideramos los estados como el conjunto de posiciones de las cámaras, y la función a minimizar será una modificación sobre la <em>métrica de error basada en oclusión</em>. Dado un estado, tiene sentido considerar a los siguientes estados posibles como aquellos que cambian la posición de exactamente una cámara.</p><p>La primera modificación necesaria es considerar múltiples puntos. Supongamos que estos son $p_1,\ldots,p_m$. Entonces denotemos $Q_i$ como la métrica descrita antes, con el punto $p_i$ como objetivo. Nos interesa minimizar</p>\[f = \sum_{i = 1}^m Q_i.\]<p>Esto aún genera un problema. ¿Por qué? La métrica original no incentiva el poner una cámara en una región que no contenga ninguna cámara: al pasar de 0 cámaras a 1 cámara en la región, esta región sigue sin contener al menos un par de cámaras, así que su longitud se sigue considerando. Más aún, este cambio empeora la métrica si la región de la cual se toma la cámara a mover contiene sólo un par de cámaras.</p><p>Para esto definimos $\theta$ como un <strong><em>penalty</em></strong> asignado a puntos que no son visibles por un par de cámaras. Consideremos un punto $p_i$, y $C_i$ como el conjunto de puntos cuyos vectores de vista ven a $p_i$. Entonces, definimos el valor $E_i$ como la nueva métrica, dada por:</p>\[E_i = \begin{cases} 360 + \theta &amp; |C_i| = 0\\ Q_i &amp; |C_i| &gt; 0. \end{cases}\]<p>Finalmente, queremos minimizar $f$, redefinida como</p>\[f = \sum_{i=1}^m E_i,\]<p>usando recocido simulado. El paper original sugiere usar $\theta = 360$ para obtener mejores resultados.</p><h1 id="el-sistema-de-motion-capture-en-cimat----guía-de-uso">El sistema de Motion Capture en CIMAT - Guía de uso</h1><p>El procedimiento anterior, aunque interesante en teoría, no es aplicado en CIMAT en la actualidad. Para el sistema de CIMAT, se cuenta con la siguiente guía:</p><h2 id="calibración"><span class="mr-2">Calibración</span><a href="#calibración" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Se utiliza el software Motion, de Optitrack; a través de este se hará la calibración.</p><h3 id="preparar-el-ambiente"><span class="mr-2">Preparar el ambiente</span><a href="#preparar-el-ambiente" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Se deben bloquear o remover los objetos que puedan interferir con las cámaras; ya sea ventanas abiertas, superficies reflejantes, luces infrarojas, marcadores, etc.</p><h3 id="camera-masking"><span class="mr-2">Camera Masking</span><a href="#camera-masking" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Se cubren las fuentes de luz restantes; incluyendo la interferencia de las cámaras entre sí. Esto se puede hacer con la opción de Auto-Masking, “Block-Visible”. Esta esconde de manera automática los puntos brillantes. Otra opción es cubrirlas manualmente con herramientas de selección.</p><p><img data-src="/Movimientos-antropomorfos/assets/images/mask1.png" alt="Masking antes 1" data-proofer-ignore></p><p><img data-src="/Movimientos-antropomorfos/assets/images/mask2.png" alt="Masking antes 2" data-proofer-ignore></p><p><img data-src="/Movimientos-antropomorfos/assets/images/mask3.png" alt="Masking despues" data-proofer-ignore></p><h3 id="wanding"><span class="mr-2">Wanding</span><a href="#wanding" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Se debe presionar el botón “Start Wanding”; el motor de calibración comenzará a grabar samples cuando detecte la varita de calibración. Se debe cubrir con los marcadores de la varita todo el volumen que detecten las cámaras. El motor de calibración se mostrará verde cuando tenga suficientes samples. Es recomendable seguir el proceso hasta cubrir suficiente área.</p><p><img data-src="/Movimientos-antropomorfos/assets/images/wand.png" alt="Varita" data-proofer-ignore></p><p><img data-src="/Movimientos-antropomorfos/assets/images/wandsoftw.png" alt="Varita Software" data-proofer-ignore></p><h3 id="calculation"><span class="mr-2">Calculation</span><a href="#calculation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Tras hacer el “Wanding”, el proceso de cálculo se hace presionando el botón “Calculate” del panel “Calibration”. Solo es necesario esperar a que el proceso converja a una solución, pero se puede dejar continuar para obtener calibración más precisa. Se puede monitorear el proceso con el visor 3D, que se ve como sigue:</p><p><img data-src="/Movimientos-antropomorfos/assets/images/calc.png" alt="Calculo" data-proofer-ignore></p><h3 id="aplicar-los-resultados"><span class="mr-2">Aplicar los resultados</span><a href="#aplicar-los-resultados" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Solo es necesario presionar el botón “Apply Results”. Saldrá un aviso de guardar los resultados del “Wanding”. Tras guardar, es posible escoger el plano del suelo con el artefacto que determina el plano Z.</p><p><img data-src="/Movimientos-antropomorfos/assets/images/resul.png" alt="Resultados" data-proofer-ignore></p><p><img data-src="/Movimientos-antropomorfos/assets/images/piso.png" alt="Objeto piso" data-proofer-ignore></p><h3 id="verificar-los-resultados"><span class="mr-2">Verificar los resultados</span><a href="#verificar-los-resultados" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Ejecutar la herramienta “Volume Accuracy Tool” para verificar la calidad de la calibración. Se mide la desviación entre las longitudes que se midieron y las reales de la varita de calibración.</p><h2 id="sesión-de-grabación"><span class="mr-2">Sesión de Grabación</span><a href="#sesión-de-grabación" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="suit-up"><span class="mr-2">Suit Up</span><a href="#suit-up" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Ponerse el traje de la talla adecuada, lo más ajustado posible; esto previene que los marcadores se muevan mucho respecto al cuerpo.</p><p>Los marcadores se deben colocar en lugares específicos (Marker Sets). El estándar, “Baseline” consiste en 37 marcadores y se puede ver en “Views”-&gt;”Sekeletons”-&gt;”Choose Markerset”.</p><p><img data-src="/Movimientos-antropomorfos/assets/images/markers.png" alt="Marcadores" data-proofer-ignore></p><h3 id="definir-un-esqueleto"><span class="mr-2">Definir un esqueleto</span><a href="#definir-un-esqueleto" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Hacer clic en “Layout”-&gt;”Create”. El actor para el cual se vaya a definir el esqueleto se debe colocar en el centro del volumen de grabación con los marcadores de el Marker Set adecuado.</p><p>El actor se debe colocar en “posición T” frente a las cámaras, y se mostrará un modelo cuando el actor este en la posición correcta.</p><p><img data-src="/Movimientos-antropomorfos/assets/images/tpose.png" alt="Pose" data-proofer-ignore></p><h3 id="grabar-datos"><span class="mr-2">Grabar datos</span><a href="#grabar-datos" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Tras calibrar y definir esqueletos, seleccionamos “Layout”-&gt; “Capture” para acceder a las opciones de grabación. Seleccionar el botón rojo de grabación y comenzar a capturar los datos 3D.</p><p>El panel de timeline funciona de manera similar que un software de grabación de audio o una cámara de video. Con el botón comenzamos a grabar en una nueva toma. El panel (botón de comenzar a grabar, detener, etc) es similar a cualquier otro software de grabación.</p><p>En la sección “Take” se da el nombre a la toma. También se muestra información como el tiempo grabado, la latencia y los “frames per second”.</p><p>Si el esqueleto pierde la forma, se puede calibrar posicionandose de nuevo en posición de T.</p><p><img data-src="/Movimientos-antropomorfos/assets/images/grabar.png" alt="Grabar" data-proofer-ignore></p><h2 id="sesión-de-edición"><span class="mr-2">Sesión de Edición</span><a href="#sesión-de-edición" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Abrir un proyecto existente y seleccionar “Layout”-&gt;”Edit”. Escoger alguna toma (“Takes”) para obtener la información de marcadores de esta.</p><p>En el panel “Project”, la sección “Assets” muestra los esqueletos y cuerpos rígidos de la toma. Al seleccionar algún esqueleto, encontramos sus marcadores, y se pueden seleccionar para Editar.</p><h3 id="eliminar-marcadores-de-ruido"><span class="mr-2">Eliminar marcadores de ruido</span><a href="#eliminar-marcadores-de-ruido" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Para cada esqueleto y cuerpo rígido se deben eliminar los marcadores que aparezcan como “Undefined”.</p><h3 id="seleccionar-animación-deseada"><span class="mr-2">Seleccionar animación deseada</span><a href="#seleccionar-animación-deseada" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>En el editor de “timeline” se selecciona la sección de grabación para trabajar, como se ve en la imagen:</p><p><img data-src="/Movimientos-antropomorfos/assets/images/select.png" alt="Seleccionar" data-proofer-ignore></p><h3 id="fill-gaps"><span class="mr-2">Fill gaps</span><a href="#fill-gaps" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Seleccionamos marcadores con “gaps” (huecos) en sus gráficas; estos se van a llenar escogiendo con alguna interpolación.</p><p>Esto se puede hacer en “Fill Gaps” del panel “Edit Tools”. Seleccionamos el tamaño máximo de los gaps que llenaremos con una interpolación, así como el tipo de interpolación a usar (ej. Max. Gap Size = 10) que utilizaremos (ej. Interpolation = Cubic). Con la opción “Fill Selected” se llenan los gaps seleccionados.</p><p><img data-src="/Movimientos-antropomorfos/assets/images/gaps.png" alt="Gaps" data-proofer-ignore></p></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://twitter.com/username">José Angel - Sebastián Sanchez - Diego García - Angel Ayala</a>. <span data-toggle="tooltip" data-placement="top" title="Salvo que se indique explícitamente, las entradas de este blog están licenciadas bajo la Creative Commons Attribution 4.0 International (CC BY 4.0) License por el autor.">Algunos derechos reservados.</span></p></div><div class="footer-right"><p class="mb-0"> Hecho con <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> usando el tema <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> .</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">Hay una nueva versión de contenido disponible.</p><button type="button" class="btn btn-primary" aria-label="Update"> Actualizar </button></div></div><script src=""></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/Movimientos-antropomorfos/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">¡Oops! No se encuentran resultados.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/Movimientos-antropomorfos/assets/js/dist/page.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/Movimientos-antropomorfos/app.js"></script>
